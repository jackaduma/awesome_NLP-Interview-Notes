
# awesome_NLP-Interview-Notes
NLP interview notes and answers

## **内容说明**

问题来自 [NLP 面无不过： km1994 - NLP-Interview-Notes ](https://github.com/km1994/NLP-Interview-Notes)

**答案 为 自己编写，不保证正确，仅供参考。**

**有些问题，提供的答案更多是一种线索，如想深入了解，请自行参考更多资料**

## 更新记录

1. 2023.11 发布：CNN, RNN, Attention

## 目录

- [x] **[3 深度学习算法篇常见面试篇](./3%20深度学习算法篇常见面试篇/)**
  - [x] **[CNN 常见面试篇](./3%20深度学习算法篇常见面试篇/CNN%20常见面试篇.md)**
    - [x] 一、动机篇
    - [x] 二、CNN 卷积层篇
      - [x] 2.1 卷积层的本质是什么？
      - [x] 2.2 CNN 卷积层与全连接层的联系？
      - [x] 2.3 channel的含义是什么？
    - [x] 三、CNN 池化层篇
      - [x] 3.1 池化层针对区域是什么？
      - [x] 3.2 池化层的种类有哪些？
      - [x] 3.3 池化层的作用是什么？
      - [x] 3.4 池化层 反向传播 是什么样的？
      - [x] 3.5 mean pooling 池化层 反向传播 是什么样的？
      - [x] 3.6 max pooling 池化层 反向传播 是什么样的？
    - [x] 四、CNN 整体篇
      - [x] 4.1 CNN 的流程是什么？
      - [x] 4.2 CNN 的特点是什么？
      - [x] 4.3 卷积神经网络为什么会具有平移不变性？
      - [x] 4.4 卷积神经网络中im2col是如何实现的？
      - [x] 4.5 CNN 的局限性是什么？
    - [x] 五、Iterated Dilated CNN 篇
      - [x] 5.1 什么是 Dilated CNN 空洞卷积？
      - [x] 5.2 什么是 Iterated Dilated CNN？
    - [x] 六、反卷积 篇
      - [x] 6.1 解释反卷积的原理和用途？
  - [x] **[RNN 常见面试篇](./3%20深度学习算法篇常见面试篇/RNN%20常见面试篇.md)**
    - [x] 一、RNN 篇
      - [x] 1.2 为什么需要 RNN?
      - [x] 1.2 RNN 结构是怎么样的？
      - [x] 1.3 RNN 前向计算公式？
      - [x] 1.4 RNN 存在什么问题？
    - [x] 二、长短时记忆网络(Long Short Term Memory Network, LSTM) 篇
      - [x] 2.1 为什么 需要 LSTM?
      - [x] 2.2 LSTM 的结构是怎么样的?
      - [x] 2.3 LSTM 如何缓解 RNN 梯度消失和梯度爆炸问题?
      - [x] 2.3 LSTM 的流程是怎么样的?
      - [x] 2.4 LSTM 中激活函数区别?
      - [x] 2.5 LSTM的复杂度？
      - [x] 2.6 LSTM 存在什么问题？
    - [x] 三、GRU (Gated Recurrent Unit)
      - [x] 3.1 为什么 需要 GRU?
      - [x] 3.2 GRU 的结构是怎么样的?
      - [x] 3.3 GRU 的前向计算?
      - [x] 3.4 GRU 与其他 RNN系列模型的区别？
    - [x] 四、RNN系列模型篇
      - [x] 4.1 RNN系列模型 有什么特点？
  - [x] **[Attention 常见面试篇](./3%20深度学习算法篇常见面试篇/Attention%20常见面试篇.md)**
    - [x] 一、seq2seq 篇
      - [x] 1.1 seq2seq （Encoder-Decoder）是什么？
      - [x] 1.2 seq2seq 中 的 Encoder 怎么样？
      - [x] 1.3 seq2seq 中 的 Decoder 怎么样？
      - [x] 1.4 在 数学角度上 的 seq2seq ，你知道么？
      - [x] 1.5 seq2seq 存在 什么 问题？
    - [x] 二、Attention 篇
      - [x] 2.1 什么是 Attention?
      - [x] 2.2 为什么引入 Attention机制？
      - [x] 2.3 Attention 有什么作用？
      - [x] 2.4 Attention 流程是怎么样？
      - [x] 2.5 Attention 的应用领域有哪些？
    - [x] 三、Attention 变体篇
      - [x] 3.1 Soft Attention 是什么？
      - [x] 3.2 Hard Attention 是什么？
      - [x] 3.3 Global Attention 是什么？
      - [x] 3.4 Local Attention 是什么？
      - [x] 3.5 self-attention 是什么？
  - [x] **[生成对抗网络 GAN 常见面试篇](./3%20深度学习算法篇常见面试篇/生成对抗网络%20GAN%20常见面试篇.md)** 
    - [x] 一、动机
    - [x] 二、介绍篇
      - [x] 2.1 GAN 的基本思想
      - [x] 2.2 GAN 基本介绍
        - [x] 2.2.1 GAN 的基本结构
        - [x] 2.2.2 GAN 的基本思想
    - [x] 三、训练篇
      - [x] 3.1 生成器介绍
      - [x] 3.2 判别器介绍
      - [x] 3.3 训练过程
      - [x] 3.4 训练所涉及相关理论基础
    - [x] 四、总结

------
## **Star-History**

![star-history](https://api.star-history.com/svg?repos=jackaduma/awesome_NLP-Interview-Notes&type=Date "star-history")

------

## Donation
If this project help you reduce time to develop, you can give me a cup of coffee :) 

AliPay(支付宝)
<div align="center">
	<img src="./misc/ali_pay.png" alt="ali_pay" width="400" />
</div>

WechatPay(微信)
<div align="center">
    <img src="./misc/wechat_pay.png" alt="wechat_pay" width="400" />
</div>

------
