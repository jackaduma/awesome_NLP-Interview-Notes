#面试 #BERT 

- [[#什么是 Bert 未登录词？|什么是 Bert 未登录词？]]
- [[#Bert 未登录词 如何处理？|Bert 未登录词 如何处理？]]
- [[#Bert 未登录词各种处理方法 有哪些优缺点？|Bert 未登录词各种处理方法 有哪些优缺点？]]

### 什么是 Bert 未登录词？
BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer架构的预训练语言模型，它在自然语言处理领域取得了巨大的成功。在BERT中，未登录词（Out-Of-Vocabulary，OOV）指的是在训练BERT模型时没有出现在词汇表中的词语。

对于未登录词，BERT使用了一种特殊的标记来表示，即"[UNK]"（unknown token）。当模型在输入文本中遇到未登录词时，会用"[UNK]"来代替该词，以表明模型无法识别该词的具体含义。

在实际应用中，为了处理未登录词，通常需要对输入文本进行预处理，将未登录词替换为"[UNK]"标记，然后再输入到BERT模型中进行处理。这样可以确保模型在处理未登录词时能够正确地进行推断和预测。

总之，BERT中的未登录词是指在训练时未出现在词汇表中的词语，模型会用特殊的"[UNK]"标记来表示这些未登录词。
### Bert 未登录词 如何处理？
处理BERT中的未登录词（Out-Of-Vocabulary，OOV）通常需要在输入文本中将未登录词替换为特殊的"[UNK]"标记，然后再输入到BERT模型中进行处理。这样可以确保模型在处理未登录词时能够正确地进行推断和预测。下面是一种常见的处理方法：

1. 预处理：在将文本输入到BERT模型之前，需要对文本进行预处理。首先，将文本分词成单词或子词（subwords），然后将未登录词替换为特殊的"[UNK]"标记。

2. 使用词块：对于一些未登录词，可以尝试将其拆分成更小的词块或子词，这样可能会使得一些未登录词能够在BERT的词汇表中找到对应的部分，从而减少未登录词的影响。

3. 使用外部词汇表：有时候可以使用外部的词汇表来扩展BERT的词汇表，以包含一些未登录词。这样可以提高模型对未登录词的处理能力。

4. Fine-tuning：在特定任务上进行微调时，可以通过在训练数据中引入未登录词的变体来训练模型，从而提高模型对未登录词的处理能力。

需要注意的是，处理未登录词的方法可能会因任务和数据集的不同而有所差异。在实际应用中，可以根据具体情况选择合适的方法来处理BERT中的未登录词。
### Bert 未登录词各种处理方法 有哪些优缺点？
处理BERT中的未登录词有多种方法，每种方法都有其优点和缺点。以下是一些常见的处理方法以及它们的优缺点：

1. 替换为特殊标记"[UNK]"：
   - 优点：简单直接，不需要额外的处理步骤。
   - 缺点：丢失了未登录词的具体信息，可能影响模型对文本的理解和推断能力。

2. 使用词块（subwords）：
   - 优点：可以将一些未登录词拆分成更小的词块或子词，使得一部分未登录词能够在BERT的词汇表中找到对应的部分。
   - 缺点：增加了文本预处理的复杂度，可能引入歧义性，影响模型对文本的理解。

3. 使用外部词汇表扩展BERT的词汇表：
   - 优点：可以提高模型对未登录词的处理能力，增加了模型对未登录词的覆盖范围。
   - 缺点：需要额外的词汇表，可能会增加模型的计算和存储开销。

4. Fine-tuning时引入未登录词的变体：
   - 优点：可以通过训练数据中引入未登录词的变体来提高模型对未登录词的处理能力。
   - 缺点：需要大量的训练数据，训练成本高，且并不总是有效。

总的来说，每种处理方法都有其适用的场景和局限性。在实际应用中，可以根据具体任务和数据集的情况选择合适的处理方法。同时，也可以尝试结合多种方法来处理BERT中的未登录词，以提高模型对未登录词的处理能力。